"""
/***************************************************************************
 Segmenter
                                 A QGIS plugin
 This plugin segments the map into discrete buckets
 Generated by Plugin Builder: http://g-sherman.github.io/Qgis-Plugin-Builder/
                              -------------------
        begin                : 2023-05-26
        git sha              : $Format:%H$
        copyright            : (C) 2023 by Quant Civil
        email                : joshua.herrera@quantcivil.ai
 ***************************************************************************/
"""
from qgis.PyQt.QtCore import (
    QSettings,
    QTranslator,
    QCoreApplication,
    QThread,
)
from qgis.PyQt.QtGui import QIcon, QPixmap
from qgis.PyQt.QtWidgets import QAction

# Initialize Qt resources from file resources.py
from .resources import *

# Import the code for the dialog
from .segmenter_dialog import SegmenterDialog
import os.path
from tempfile import gettempdir

from qgis.core import (
    QgsApplication,
    QgsRasterLayer,
    QgsProject
)

from io import BytesIO
import requests
from osgeo import gdal

from .keygen import activate_license
import torch
from sklearn.cluster import KMeans
import numpy as np
import cv2

# Class for training thread
class Trainer(QObject):
    finished = pyqtSignal()

    def __init__(
        self, model, epochs, data, tile_size, pixel_size, y_tiles, x_tiles, device, key
    ):
        super().__init__()
        self.model = model
        self.epochs = epochs
        self.data = data
        self.tile_size = tile_size
        self.pixel_size = pixel_size
        self.y_tiles = y_tiles
        self.x_tiles = x_tiles
        self.device = device
        self.key = key
        self.ip = "0.0.0.0"

    def run(self):
        # Format package
        package = {}

        # Create image for transmission
        image_data = np.empty(
            (self.tile_size * self.y_tiles, self.tile_size * self.x_tiles, 3),
            dtype="uint8",
        )
        for yy in range(self.y_tiles):
            for xx in range(self.x_tiles):
                image_data[
                    yy * self.tile_size : yy * self.tile_size + self.tile_size,
                    xx * self.tile_size : xx * self.tile_size + self.tile_size,
                ] = self.data[xx + yy * self.x_tiles]
        data_buffer = cv2.imencode(".jpg", image_data)[1]
        package["data"] = data_buffer.tobytes().decode("latin-1")

        # Rest of the stuff
        model_buffer = BytesIO()
        torch.save(self.model.state_dict(), model_buffer)
        package["model_state"] = model_buffer.getvalue().decode("latin-1")
        package["epochs"] = self.epochs
        package["tile_size"] = self.tile_size
        package["x_tiles"] = self.x_tiles
        package["y_tiles"] = self.y_tiles
        package["pixel_size"] = self.pixel_size
        package["key"] = self.key

        url = "http://qgis.quantcivil.ai:5000/train"
        response = requests.post(url, json=package, timeout=1e6)
        if response.status_code == 200:
            self.ip = response.text
        else:
            self.ip = -1

        self.finished.emit()


# Class for segmenting thread
class Predictor(QObject):
    finished = pyqtSignal()

    def __init__(
        self,
        model,
        data,
        tile_size,
        pixel_size,
        y_tiles,
        x_tiles,
        device,
        clusters,
        key,
    ):
        super().__init__()
        self.model = model
        self.data = data
        self.tile_size = tile_size
        self.pixel_size = pixel_size
        self.y_tiles = y_tiles
        self.x_tiles = x_tiles
        self.device = device
        self.clusters = clusters
        self.key = key
        self.ip = "0.0.0.0"

    def run(self):
        # Format package
        package = {}

        # Create image for transmission
        image_data = np.empty(
            (self.tile_size * self.y_tiles, self.tile_size * self.x_tiles, 3),
            dtype="uint8",
        )
        for yy in range(self.y_tiles):
            for xx in range(self.x_tiles):
                image_data[
                    yy * self.tile_size : yy * self.tile_size + self.tile_size,
                    xx * self.tile_size : xx * self.tile_size + self.tile_size,
                ] = self.data[yy, xx]
        data_buffer = cv2.imencode(".jpg", image_data)[1]
        package["data"] = data_buffer.tobytes().decode("latin-1")

        # Rest of the stuff
        model_buffer = BytesIO()
        torch.save(self.model.state_dict(), model_buffer)
        package["model_state"] = model_buffer.getvalue().decode("latin-1")
        package["tile_size"] = self.tile_size
        package["x_tiles"] = self.x_tiles
        package["y_tiles"] = self.y_tiles
        package["pixel_size"] = self.pixel_size
        package["clusters"] = self.clusters
        package["key"] = self.key

        url = "http://qgis.quantcivil.ai:5000/predict"
        response = requests.post(url, json=package, timeout=1e6)
        if response.status_code == 200:
            self.ip = response.text
        else:
            self.ip = -1

        self.finished.emit()


class Segmenter:
    """QGIS Plugin Implementation."""

    def __init__(self, iface):
        """Constructor.

        :param iface: An interface instance that will be passed to this class
            which provides the hook by which you can manipulate the QGIS
            application at run time.
        :type iface: QgsInterface
        """
        # Save reference to the QGIS interface
        self.iface = iface
        # initialize plugin directory
        self.plugin_dir = os.path.dirname(__file__)
        # initialize locale
        locale = QSettings().value("locale/userLocale")[0:2]
        locale_path = os.path.join(
            self.plugin_dir, "i18n", "Segmenter_{}.qm".format(locale)
        )

        if os.path.exists(locale_path):
            self.translator = QTranslator()
            self.translator.load(locale_path)
            QCoreApplication.installTranslator(self.translator)

        # Declare instance attributes
        self.actions = []
        self.menu = self.tr("&Map Segmenter")

        # Check if plugin was started the first time in current QGIS session
        # Must be set in initGui() to survive plugin reloads
        self.first_start = None

        QSettings().setValue("/qgis/parallel_rendering", True)
        threadcount = QThread.idealThreadCount()
        QgsApplication.setMaxThreads(threadcount)

        self.keygen_account_id = "ae9bc51d-ae1c-482e-9223-c4243cd7e434"

    # noinspection PyMethodMayBeStatic
    def tr(self, message):
        """Get the translation for a string using Qt translation API.

        We implement this ourselves since we do not inherit QObject.

        :param message: String for translation.
        :type message: str, QString

        :returns: Translated version of message.
        :rtype: QString
        """
        # noinspection PyTypeChecker,PyArgumentList,PyCallByClass
        return QCoreApplication.translate("Segmenter", message)

    def add_action(
        self,
        icon_path,
        text,
        callback,
        enabled_flag=True,
        add_to_menu=True,
        add_to_toolbar=True,
        status_tip=None,
        whats_this=None,
        parent=None,
    ):
        """Add a toolbar icon to the toolbar.

        :param icon_path: Path to the icon for this action. Can be a resource
            path (e.g. ':/plugins/foo/bar.png') or a normal file system path.
        :type icon_path: str

        :param text: Text that should be shown in menu items for this action.
        :type text: str

        :param callback: Function to be called when the action is triggered.
        :type callback: function

        :param enabled_flag: A flag indicating if the action should be enabled
            by default. Defaults to True.
        :type enabled_flag: bool

        :param add_to_menu: Flag indicating whether the action should also
            be added to the menu. Defaults to True.
        :type add_to_menu: bool

        :param add_to_toolbar: Flag indicating whether the action should also
            be added to the toolbar. Defaults to True.
        :type add_to_toolbar: bool

        :param status_tip: Optional text to show in a popup when mouse pointer
            hovers over the action.
        :type status_tip: str

        :param parent: Parent widget for the new action. Defaults None.
        :type parent: QWidget

        :param whats_this: Optional text to show in the status bar when the
            mouse pointer hovers over the action.

        :returns: The action that was created. Note that the action is also
            added to self.actions list.
        :rtype: QAction
        """

        icon = QIcon(icon_path)
        action = QAction(icon, text, parent)
        action.triggered.connect(callback)
        action.setEnabled(enabled_flag)

        if status_tip is not None:
            action.setStatusTip(status_tip)

        if whats_this is not None:
            action.setWhatsThis(whats_this)

        if add_to_toolbar:
            # Adds plugin icon to Plugins toolbar
            self.iface.addToolBarIcon(action)

        if add_to_menu:
            self.iface.addPluginToMenu(self.menu, action)

        self.actions.append(action)

        return action

    def initGui(self):
        """Create the menu entries and toolbar icons inside the QGIS GUI."""

        icon_path = ":/plugins/segmenter/icon.png"
        self.add_action(
            icon_path,
            text=self.tr("Segment the map"),
            callback=self.run,
            parent=self.iface.mainWindow(),
        )

        # will be set False in run()
        self.first_start = True

    def unload(self):
        """Removes the plugin menu item and icon from QGIS GUI."""
        for action in self.actions:
            self.iface.removePluginMenu(self.tr("&Map Segmenter"), action)
            self.iface.removeToolBarIcon(action)

    # Get the map image within the bounding box provided
    def get_input(self, bounding_box):
        settings = self.iface.mapCanvas().mapSettings()
        settings.setExtent(bounding_box)
        settings.setOutputSize(QSize(self.tile_size, self.tile_size))
        renderer = QgsMapRendererParallelJob(settings)

        event_loop = QEventLoop()
        renderer.finished.connect(event_loop.quit)
        renderer.start()
        event_loop.exec_()

        img = renderer.renderedImage()
        img = qimage2ndarray.rgb_view(img)
        img = cv2.resize(img, (self.tile_size, self.tile_size))
        return img

    def finished_remote_train(self):
        if self.trainer.ip == -1:
            self.dlg.inputKey.setPlainText(f"Bees are busy, try again in a moment.")
        else:
            self.dlg.inputKey.setPlainText(f"Finished uploading to {self.trainer.ip}!")
        self.tthread = None

    # Refresh button
    def update_progress(self):

        if self.worker is None:
            self.dlg.inputKey.setPlainText("Nothing offloaded, try offloading a job.")
            return

        if self.worker.ip == "0.0.0.0":
            self.dlg.inputKey.setPlainText("Hold your horses üê¥\nWe're still uploading.")
            return
        elif self.worker.ip == -1:
            self.dlg.inputKey.setPlainText("Bees are busy, try again in a moment.")
            self.worker = None
            return

        url = "http://qgis.quantcivil.ai:5000/watcher/" + self.worker.ip
        progress = int(requests.get(url).text)
        self.dlg.progressBar.setValue(progress)

        # Fun little progress updates
        if progress == 0:
            self.dlg.inputKey.setPlainText("Annihilating the launchpad üöÄ")
        elif progress < 10:
            self.dlg.inputKey.setPlainText("Fighting gravity üçé")
        elif progress < 20:
            self.dlg.inputKey.setPlainText("Avoiding a deathspiral üíÄ")
        elif progress < 30:
            self.dlg.inputKey.setPlainText("Entering low earth orbit üåé")
        elif progress < 40:
            self.dlg.inputKey.setPlainText("Readying the bits üë©‚Äçüíª")
        elif progress < 50:
            self.dlg.inputKey.setPlainText("Performing experiments üî¨")
        elif progress < 60:
            self.dlg.inputKey.setPlainText("Fighting thermal fallout ‚ò¢")
        elif progress < 70:
            self.dlg.inputKey.setPlainText("Crunching numbers üî¢")
        elif progress < 80:
            self.dlg.inputKey.setPlainText("Packing up üì¶")
        elif progress < 90:
            self.dlg.inputKey.setPlainText("Beaming down üññ")
        elif progress < 100:
            self.dlg.inputKey.setPlainText("Wrapping up üéÅ")
        else:
            self.dlg.inputKey.setPlainText("The eagle has landed ü¶Ö")

        if progress == 100:
            url = "http://qgis.quantcivil.ai:5000/payload/" + self.worker.ip
            payload = requests.get(url)
            if payload.status_code == 400:
                self.dlg.inputKey.setPlainText("Error, try again.")
                self.worker.ip = -1
                return
            elif payload.status_code == 201:  # model
                state_dict = torch.load(
                    BytesIO(payload.text.encode("latin-1")),
                    weights_only=True,
                    map_location=self.device,
                )
                self.set_model(state_dict)
                self.dlg.inputKey.setPlainText("Mission success ‚úÖ\nModel loaded")
            elif payload.status_code == 200:  # raster
                data = np.asarray(
                    bytearray(payload.text.encode("latin-1")), dtype="uint8"
                )
                data = cv2.imdecode(data, cv2.IMREAD_GRAYSCALE)
                self.write_raster_layer(data, self.bounding_box)
                self.dlg.inputKey.setPlainText("Mission success ‚úÖ\nRaster loaded")

            self.worker.ip == -1

    # Train model on map data
    def train(self):
        time_enum = {
            "very short": 5,
            "short": 25,
            "medium": 50,
            "long": 100,
            "very long": 500,
        }

        # Instantiate kmeans model
        kmeans = KMeans(n_clusters=num_segments)

        # Get user specified resolution
        resolution = resolution_map[self.dlg.inputRes.currentText()]

        # Set up model stuff
        epochs = time_enum[self.dlg.inputTrainingTime.currentText()]
        optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-5)
        criterion = torch.nn.L1Loss()

        # Reshape into 2d
        array_2d = array_padded.reshape(
            array_padded.shape[0],
            array_padded.shape[1] // resolution,
            resolution,
            array_padded.shape[2] // resolution,
            resolution,
        )
        array_2d = array_2d.transpose(1, 3, 0, 2, 4)
        array_2d = array_2d.reshape(
            array_2d.shape[0] * array_2d.shape[1],
            array_2d.shape[2] * resolution * resolution
        )

        # Fit kmeans model
        kmeans = kmeans.fit(array_2d)

        # Get clusters
        clusters = kmeans.labels_

        # Reshape clusters to match map
        clusters = clusters.reshape(
            1, 1,
            array.shape[1] // resolution,
            array.shape[2] // resolution,
        )

        # Get rid of padding
        clusters = clusters[:, :, :array.shape[1] // resolution, :array.shape[2] // resolution]

        # Upsample to original size
        clusters = torch.tensor(clusters).to(self.device)
        clusters = torch.nn.Upsample(size=(array.shape[-2], array.shape[-1]), mode="nearest")(clusters)
        clusters = clusters[0]

        return clusters.cpu().numpy()

    # Predict coverage map using cnn
    def predict_cnn(self, array):

        # Print message about gpu
        if self.device == torch.device("cpu"):
            self.dlg.inputBox.setPlainText("WARNING: GPU not available. Using CPU instead.")

        assert array.shape[0] == 3, f"Invalid array shape! \n{array.shape}"

        # Download and load model
        model_bytes = self.keygen_model(
            self.dlg.inputRes.currentText(),
            self.key
        )
        cnn_model = torch.jit.load(model_bytes).to(self.device)

        # Pad to tile_size
        channel_pad = (0, 0)
        height_pad = (0, TILE_SIZE - array.shape[1] % TILE_SIZE)
        width_pad = (0, TILE_SIZE - array.shape[2] % TILE_SIZE)
        array_padded = np.pad(
            array,
            (channel_pad, height_pad, width_pad),
            mode="constant",
        )

        # Reshape into tiles
        tiles = array_padded.reshape(
            3,
            array_padded.shape[1] // TILE_SIZE,
            TILE_SIZE,
            array_padded.shape[2] // TILE_SIZE,
            TILE_SIZE,
        )
        tiles = tiles.transpose(1, 3, 0, 2, 4)
        tiles = tiles.reshape(
            tiles.shape[0] * tiles.shape[1],
            3,
            TILE_SIZE,
            TILE_SIZE,
        )

        # Convert to float range [0, 1]
        tiles = tiles.astype("float32") / 255

        # Convert to torch
        tiles = torch.from_numpy(tiles).to(self.device)
        
        # Write some tiles to disk
        # for i in range(0, tiles.shape[0], 100):
        #     tile = tiles[i]
        #     tile = tile.cpu().numpy()
        #     tile = tile.transpose(1, 2, 0)
        #     tile = tile * 255
        #     tile = tile.astype("uint8")
        #     cv2.imwrite(f"/Volumes/storage/gits/quant-civil/segmenter/images/tile_{i}.png", tile)

        # Predict classes
        batch_size = 16
        coverage_map = []
        for i in range(0, tiles.shape[0], batch_size):
            vectors = cnn_model.forward(tiles[i:i+batch_size])
            vectors = torch.exp(vectors)
            indices = torch.argmax(vectors, dim=1).type(torch.uint8)
            coverage_map.append(indices)
        coverage_map = torch.concatenate(coverage_map, dim=0)
        coverage_map = coverage_map.cpu().numpy()

        # Convert from tiles to one big map
        coverage_map = coverage_map.reshape(
            array_padded.shape[1] // TILE_SIZE,
            array_padded.shape[2] // TILE_SIZE,
            coverage_map.shape[1],
            coverage_map.shape[2],
        )
        coverage_map = coverage_map.transpose(0, 2 ,1, 3)
        coverage_map = coverage_map.reshape(
            1, 1,
            coverage_map.shape[0] * coverage_map.shape[1],
            coverage_map.shape[2] * coverage_map.shape[3],
        )

        # Convert to torch for gpu
        coverage_map = torch.from_numpy(coverage_map).to(self.device)

        # Upsample to original size
        coverage_map = torch.nn.Upsample(size=(array_padded.shape[-2], array_padded.shape[-1]), mode="nearest")(coverage_map)

        # Get rid of padding
        coverage_map = coverage_map[0, :, :array.shape[1], :array.shape[2]]

        return coverage_map.cpu().numpy()

        # Transfer weights to encoder
        self.encoder.load_state_dict(self.model.state_dict())

        self.dlg.progressBar.setValue(100)
        self.dlg.inputKey.setPlainText("Model trained!")

        return

    # Write 2d raster layer to canvas
    def write_raster_layer(self, raster_data, bounding_box):
        channels = 1
        raster_data = np.expand_dims(raster_data, axis=-1)

        driver = osgdal.GetDriverByName("GTiff")
        dataset = driver.Create(
            os.path.join(gettempdir(), layer_name+".tif"),
            array.shape[2],
            array.shape[1],
            array.shape[0],
            gdal.GDT_Byte,
        )

        out_srs = gdal.osr.SpatialReference()
        out_srs.ImportFromEPSG(self.canvas.layer(0).crs().postgisSrid())

        dataset.SetProjection(out_srs.ExportToWkt())

        dataset.SetGeoTransform(
            (
                bounding_box.xMinimum(),  # 0
                bounding_box.width() / array.shape[2],  # 1
                0,  # 2
                bounding_box.yMaximum(),  # 3
                0,  # 4
                -bounding_box.height() / array.shape[1],
            )
        )

        for c in range(array.shape[0]):
            dataset.GetRasterBand(c + 1).WriteArray(
                array[c, :, :]
            )
        dataset = None

        raster_layer = QgsRasterLayer(
            os.path.join(gettempdir(), layer_name+".tif"), layer_name
        )
        raster_layer.renderer().setOpacity(0.5)

        QgsProject.instance().addMapLayer(raster_layer, True)

    def finished_remote_predict(self):
        self.dlg.inputKey.setPlainText(f"Finished uploading to {self.predictor.ip}!")
        self.pthread = None

    # Create raster layer from prediction
    def predict(self):

        # Load user specified raster
        layer_name = self.dlg.inputLayer.currentText()
        layer = QgsProject.instance().mapLayersByName(layer_name)[0]
        assert layer.isValid(), f"Invalid raster layer! \n{layer_name}"
        raster = gdal.Open(layer.source())
        map_array = raster.ReadAsArray()

        # Get user specified num segments
        num_segments = int(self.dlg.inputSegments.text())

        y = self.canvas.extent().yMaximum()
        x = self.canvas.extent().xMinimum()
        upper_left = QgsPointXY(x, y)
        y = y - self.processing_scale * y_tiles
        x = x + self.processing_scale * x_tiles
        lower_right = QgsPointXY(x, y)
        self.bounding_box = QgsRectangle(upper_left, lower_right)

        # Render and tile map
        self.dlg.inputKey.setPlainText("Rendering map...\nPlease don't move the canvas")
        tiles = np.empty(
            (y_tiles, x_tiles, self.tile_size, self.tile_size, 3), dtype="uint8"
        )
        for y_tile in range(y_tiles):
            self.dlg.progressBar.setValue(100 * y_tile // y_tiles)
            for x_tile in range(x_tiles):
                # Render and tile map
                y = self.canvas.extent().yMaximum() - y_tile * self.processing_scale
                x = self.canvas.extent().xMinimum() + x_tile * self.processing_scale
                upper_left = QgsPointXY(x, y)
                y = y - self.processing_scale
                x = x + self.processing_scale
                lower_right = QgsPointXY(x, y)
                bounding_box = QgsRectangle(upper_left, lower_right)
                tiles[y_tile, x_tile] = self.get_input(bounding_box)
        self.dlg.progressBar.setValue(100)

        # If client requests online prediction
        if self.dlg.inputUseServer.isChecked():
            if self.key == "nokey":
                self.dlg.inputKey.setPlainText(
                    "Offload feature requires key\nPlease input key with dashes."
                )
                return
            if self.pthread is None:
                self.dlg.inputKey.setPlainText("Uploading assets...")
                self.pthread = QThread()
                self.predictor = Predictor(
                    self.model,
                    tiles,
                    self.tile_size,
                    self.pixel_size,
                    y_tiles,
                    x_tiles,
                    self.device,
                    clusters,
                    self.key,
                )
                self.predictor.moveToThread(self.pthread)
                self.worker = self.predictor
                self.pthread.started.connect(self.predictor.run)
                self.predictor.finished.connect(self.pthread.quit)
                self.predictor.finished.connect(self.predictor.deleteLater)
                self.pthread.finished.connect(self.pthread.deleteLater)
                self.pthread.finished.connect(self.finished_remote_predict)
                self.pthread.start()
                return

        # Execute network
        tiles = np.swapaxes(tiles, -1, -2)
        tiles = np.swapaxes(tiles, -2, -3)
        self.dlg.inputKey.setPlainText("Encoding... {}".format(message))
        batch_size = 1
        test_vector = self.encoder.forward(
            torch.tensor(tiles[0, 0], dtype=torch.float32).to(self.device)
        )
        vectors = np.empty((y_tiles, x_tiles, *test_vector.shape[-3:]), dtype="uint8")
        for yy in range(0, y_tiles):
            self.dlg.progressBar.setValue(100 * yy // y_tiles)
            for xx in range(0, x_tiles, batch_size):
                batch = tiles[yy, xx : xx + batch_size]
                batch = torch.tensor(batch, dtype=torch.float32).to(self.device)
                vectors[yy, xx : xx + batch_size] = (
                    self.encoder.forward(batch / 255).cpu().detach().numpy() * 255
                )

        # K means clustering
        self.dlg.inputKey.setPlainText("Segmenting...")
        self.dlg.progressBar.setValue(0)
        kmeans = cluster.KMeans(n_clusters=clusters)
        vectors = np.swapaxes(vectors, -3, -2)
        vectors = np.swapaxes(vectors, -2, -1)
        kmeans.fit(
            vectors.reshape(
                (
                    vectors.shape[-3] * vectors.shape[-2] * y_tiles * x_tiles,
                    vectors.shape[-1],
                )
            )
        elif self.model == "cnn":
            # Verify key
            if self.key == "nokey":
                self.dlg.inputBox.setPlainText("Please input key with dashes to use CNN model")
                return
            vectors = self.predict_cnn(map_array)
            coverage = self.predict_kmeans(vectors, num_segments=num_segments)
        else:
            self.dlg.inputBox.setPlainText("Please select a model")
            return

        # Render coverage map
        self.render_raster(
            coverage,
            layer.extent(),
            layer_name+"_coverage"
        )

        # Get mask from kmeans
        self.dlg.inputKey.setPlainText("Processing...")
        masks = np.zeros(
            (clusters, y_tiles * x_tiles * vectors.shape[-3] * vectors.shape[-2]),
            dtype="uint8",
        )
        for c in range(clusters):
            masks[c][kmeans.labels_ == c] = 255
        masks = masks.reshape((clusters, y_tiles, x_tiles, *vectors.shape[-3:-1]))

        # Process segment map
        segments = np.zeros(
            (y_tiles, x_tiles, self.tile_size, self.tile_size), dtype="uint8"
        )
        for y in range(y_tiles):
            for x in range(x_tiles):
                for c in range(clusters):
                    upscaled = cv2.resize(
                        masks[c, y, x],
                        (self.tile_size, self.tile_size),
                        cv2.INTER_LINEAR,
                    )
                    segments[y, x][upscaled > 0] = c * (255 // clusters)

        # Consolidate tiles into one big raster
        raster_data = np.empty(
            (self.tile_size * y_tiles, self.tile_size * x_tiles), dtype="uint8"
        )
        for yy in range(y_tiles):
            for xx in range(x_tiles):
                raster_data[
                    yy * self.tile_size : yy * self.tile_size + self.tile_size,
                    xx * self.tile_size : xx * self.tile_size + self.tile_size,
                ] = segments[yy, xx]

        self.write_raster_layer(raster_data, self.bounding_box)
        self.dlg.progressBar.setValue(100)
        self.dlg.inputKey.setPlainText("Map segmented!")

        return

    # Download model from keygen
    def keygen_model(self, model_name, key):
        # Get license activation token
        url = "https://api.keygen.sh/v1/accounts/{}/me".format(self.keygen_account_id)
        headers = {
            "Authorization": "License {}".format(key),
            "Accept": "application/vnd.api+json",
        }
        response = requests.get(url, headers=headers).json()
        token = response["data"]["attributes"]["metadata"]["token"]
        if "data" not in response.keys():
            raise ValueError(f"Invalid key! \n{response}")

        # Get redirect url for artifact
        url = "https://api.keygen.sh/v1/accounts/{}/artifacts/{}".format(
            self.keygen_account_id, model_name
        )
        headers = {
            "Authorization": "Bearer {}".format(token),
            "Accept": "application/vnd.api+json",
        }
        response = requests.get(url, headers=headers, allow_redirects=False).json()
        if "data" not in response.keys():
            raise ValueError(f"Invalid response from model server! \n{response}")
        redirect = response["data"]["links"]["redirect"]

        file_data = requests.get(redirect).content

        return BytesIO(file_data)

    # Set model paramters
    def reset_model(self):
        res_enum = {"low": 64, "medium": 32, "high": 16, "high+": 8, "very high": 4}

        if self.tthread is not None or self.pthread is not None:
            self.dlg.inputKey.setPlainText("Please don't alter model settings mid-run!")
            return

        input_res = self.dlg.inputResolution.currentText()
        self.pixel_size = res_enum[input_res]
        self.tile_size = 512

        self.model = AE(
            input_shape=(self.tile_size, self.tile_size),
            in_channels=3,
            pixel_size=self.pixel_size,
            decode=True,
        ).to(self.device)
        self.encoder = AE(
            input_shape=(self.tile_size, self.tile_size),
            in_channels=3,
            pixel_size=self.pixel_size,
            decode=False,
        ).to(self.device)

        self.dlg.inputKey.setPlainText("Model reset!")

    # Set model weights
    def set_model(self, state_dict):
        self.model.load_state_dict(state_dict)
        self.encoder.load_state_dict(state_dict)

    # Process user input box
    def submit(self):

        key = self.dlg.inputBox.toPlainText()
        license_key = os.path.join(self.plugin_dir, "qgis_key")

        # Process special input
        if key == "delete_key":
            os.remove(license_key)
            self.dlg.inputBox.setPlainText("Key deleted!")
            return

        # Make sure entered key looks like a key
        if len(key.split("-")) != 4:
            return
        if len(key.split("-")[-1]) < 4:
            return

        # Check existing key
        if os.path.exists(license_key):
            with open(license_key, "r") as f:
                file_key = f.readline()
            if file_key != key:
                os.remove(license_key)

        # activate license
        activated, msg = activate_license(key, self.keygen_account_id)
        if activated:
            with open(license_key, "x") as f:
                f.write(key)
            self.dlg.inputBox.setPlainText(f"Valid: {msg}")
            self.key = key
        else:
            self.dlg.inputBox.setPlainText(f"Invalid: {msg}")
            self.key = "nokey"

    #  Display models in dropdown
    def render_models(self):
        model_list = ["K-Means", "CNN"]
        self.dlg.inputLoadModel.clear()
        for model in model_list:
            self.dlg.inputLoadModel.addItem(model)

    # Download model from repo
    def download_model(self, model_path):
        # Download all remote models
        if self.key != "nokey":
            try:
                model_buffer = self.keygen_model(
                    os.path.basename(model_path), self.key
                )
            except ValueError as e:
                self.dlg.inputKey.setPlainText(f"Downloading models failed\n{e}")
                return
            with open(model_path, "wb") as f:
                f.write(model_buffer.read())
        else:
            self.dlg.inputKey.setPlainText(f"Downloading models requires key.")

    # Display resolutions in dropdown
    def render_resolutions(self):
        res_list = ["very high", "high", "medium", "low"]
        self.dlg.inputRes.clear()
        for res in res_list:
            self.dlg.inputRes.addItem(str(res))

        model_path = os.path.join(self.plugin_dir, model_name)
        if not os.path.exists(model_path):
            self.download_model(model_path)

        model = torch.load(
            model_path, map_location=self.device
        )

        try:
            self.set_model(model.state_dict())
        except RuntimeError:
            self.dlg.inputKey.setPlainText(
                f"Oops! Something went wrong. Try selecting a different resolution for this model."
            )
            self.render_models()
            return

        self.dlg.inputKey.setPlainText(f"{model_name} loaded!")

    # Save model to local disk
    def save_model(self):
        model_path = self.dlg.inputSaveModel.toPlainText() + ".torch"
        torch.save(self.model, os.path.join(self.plugin_dir, model_path))
        self.dlg.inputKey.setPlainText(f"Saved model {model_path}")

    # Check to see if server is up
    def check_server(self):

        url = "http://qgis.quantcivil.ai:5000/buzz"
        available = True
        try:
            response = requests.get(url)
            if response.status_code != 200:
                available = False
        except:
            available = False
        
        if not available:
            self.dlg.inputKey.setPlainText("Offload server offline.")
            self.dlg.inputUseServer.clicked.disconnect()
            self.dlg.inputUseServer.setChecked(0)
            self.dlg.inputUseServer.clicked.connect(self.check_server)     

    def run(self):
        """Run method that performs all the real work"""

        # Create the dialog with elements (after translation) and keep reference
        # Only create GUI ONCE in callback, so that it will only load when the plugin is started
        if self.first_start == True:
            self.first_start = False
            self.dlg = SegmenterDialog()
            self.canvas = self.iface.mapCanvas()

            # use gpu if available
            if torch.cuda.is_available():
                self.device = torch.device("cuda")
            elif torch.backends.mps.is_available():
                self.device = torch.device("mps")
            else:
                self.device = torch.device("cpu")

            # Populate drop down menus
            self.render_models()
            self.render_layers()
            self.render_resolutions()

            # Set gpu message
            gpu_msg = "GPU available."
            if self.device == torch.device("cpu"):
                gpu_msg = "GPU not available. Using CPU instead."

            # Set up license
            self.license_path = os.path.join(self.plugin_dir, "qgis_key")
            self.key = "nokey"
            if not os.path.exists(self.license_path):
                self.dlg.inputKey.setPlainText(f"Please input key with dashes\n{gpu_msg}")
            else:
                # Check license
                with open(self.license_path, "r") as f:
                    self.key = f.readline()
                act, msg = activate_license(self.key, self.keygen_account_id)
                if not act:
                    os.remove(self.license_path)
                    self.key = "nokey"
                self.dlg.inputKey.setPlainText(f"{msg}\n{gpu_msg}")

            # Render logo
            img_path = os.path.join(self.plugin_dir, "logo.png")
            pix = QPixmap(img_path)
            self.dlg.imageLarge.setPixmap(pix)

        # show the dialog
        self.dlg.show()
